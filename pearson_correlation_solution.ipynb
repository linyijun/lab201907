{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training and testing review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "\n",
    "# define file path\n",
    "train_file = os.path.join(data_path, 'review_train.csv')\n",
    "\n",
    "# read datasets\n",
    "train_data = pd.read_csv(train_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating business pairs for the Pearson correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that the similarity of (item1, item2) is the same as (item2, item1)\n",
    "# reduce the number of pairs in order to reduce the computation\n",
    "\n",
    "businesses = list(set(train_data['bid']))\n",
    "business_pairs = []\n",
    "\n",
    "for i in range(len(businesses)):\n",
    "    for j in range((i + 1), len(businesses)):\n",
    "        business_pairs.append(sorted([businesses[i], businesses[j]]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sKA6EOpxvBtCg7Ipuhl1RQ', 'xDvl_i1g1HCJ4EmIRmwfqg'],\n",
       " ['RNi6tW22UMgHwWLAb0mYdA', 'sKA6EOpxvBtCg7Ipuhl1RQ'],\n",
       " ['e2ApirIzYID9xIye0r_gKQ', 'sKA6EOpxvBtCg7Ipuhl1RQ'],\n",
       " ['WXSsJIO_uGGSxS9qC8x1gQ', 'sKA6EOpxvBtCg7Ipuhl1RQ'],\n",
       " ['l_GV0hgEoTUf70uJVT0_hg', 'sKA6EOpxvBtCg7Ipuhl1RQ']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the weight matrix (Pearson similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the review data into a dictionary {(uid, bid): rating}\n",
    "review_dict = {(row[0], row[1]): row[2] for row in train_data.values.tolist()}\n",
    "\n",
    "# map each business_id to a list of users rated on that {business_id: list[user_ids]}\n",
    "business_groups = train_data.groupby('bid')\n",
    "business_user_dict = {bid: list(business_groups.get_group(bid)['uid']) for bid in business_groups.groups}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to find common (co-rated) user list for two given businesses\n",
    "\n",
    "def find_co_rated_users(bid_1, bid_2):\n",
    "        \n",
    "    user_list_1 = business_user_dict.get(bid_1, [])\n",
    "    user_list_2 = business_user_dict.get(bid_2, [])\n",
    "    co_rated_users = list(set(user_list_1).intersection(set(user_list_2)))\n",
    "    return co_rated_users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compute Pearson correlation\n",
    "\n",
    "import math\n",
    "\n",
    "def compute_pearson_correlation(rating_list_1, rating_list_2):\n",
    "\n",
    "    n_ele = len(rating_list_1)\n",
    "    avg_rating_1 = float(sum(rating_list_1))/float(len(rating_list_1))\n",
    "    avg_rating_2 = float(sum(rating_list_2))/float(len(rating_list_2))\n",
    "\n",
    "    var_star_1 = [x - avg_rating_1 for x in rating_list_1]\n",
    "    var_star_2 = [x - avg_rating_2 for x in rating_list_2]\n",
    "    weight_sum, weight_1, weight_2 = 0.0, 0.0, 0.0\n",
    "\n",
    "    for i in range(n_ele):\n",
    "        weight_sum += var_star_1[i] * var_star_2[i]\n",
    "        weight_1 += var_star_1[i] * var_star_1[i]\n",
    "        weight_2 += var_star_2[i] * var_star_2[i]\n",
    "\n",
    "    if weight_1 == 0.0 or weight_2 == 0.0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return weight_sum / math.sqrt(weight_1) / math.sqrt(weight_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for computing weight matrix = 18.731312036514282.\n"
     ]
    }
   ],
   "source": [
    "# compute the Pearson correlation for each business pair\n",
    "\n",
    "import time\n",
    "\n",
    "weight_matrix = []\n",
    "PEARSON_THRED = 0.1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for pair in business_pairs:\n",
    "    \n",
    "    bid_1, bid_2 = pair[0], pair[1]\n",
    "    \n",
    "    # fine the co-rated users\n",
    "    co_rated_users = find_co_rated_users(bid_1, bid_2) \n",
    "    \n",
    "    if len(co_rated_users) <= 1:\n",
    "        continue\n",
    "    \n",
    "    # get the rating list of the co-rated users\n",
    "    rating_list_1 = [review_dict[(u, bid_1)]for u in co_rated_users]\n",
    "    rating_list_2 = [review_dict[(u, bid_2)] for u in co_rated_users]\n",
    "\n",
    "    # compute the Pearson correlation\n",
    "    weight = compute_pearson_correlation(rating_list_1, rating_list_2)\n",
    "\n",
    "    if weight > PEARSON_THRED:  # you can set some threshold to filter the low correlated values\n",
    "        weight_matrix.append([bid_1, bid_2, weight])\n",
    "\n",
    "print('Time for computing weight matrix = {}.'.format(time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['WXSsJIO_uGGSxS9qC8x1gQ', 'sKA6EOpxvBtCg7Ipuhl1RQ', 0.9999999999999999],\n",
       " ['l_GV0hgEoTUf70uJVT0_hg', 'sKA6EOpxvBtCg7Ipuhl1RQ', 0.1720618004029213],\n",
       " ['GJ_bXUPv672YwNg4TneJog', 'sKA6EOpxvBtCg7Ipuhl1RQ', 0.30151134457776363],\n",
       " ['Yqgyx8SJ5SqFYc-4yH6Z1g', 'sKA6EOpxvBtCg7Ipuhl1RQ', 0.8660254037844385],\n",
       " ['-95mbLJsa0CxXhpaNL4LvA', 'sKA6EOpxvBtCg7Ipuhl1RQ', 0.7071067811865475]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results to a CSV file\n",
    "\n",
    "weight_matrix_df = pd.DataFrame(weight_matrix, columns=['bid1', 'bid2', 'corr'])\n",
    "weight_matrix_file_path = os.path.join(data_path, 'weight_matrix.csv')\n",
    "weight_matrix_df.to_csv(weight_matrix_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
